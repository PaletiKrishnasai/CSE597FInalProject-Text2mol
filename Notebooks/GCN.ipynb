{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3da40b13e3034295b28e357e992ffd92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0c50ee4f3d64230b95d28b3a384cfdf","IPY_MODEL_37822fdbd8eb405892b2a6b5e780e304","IPY_MODEL_ce926a73a41c4262957ec1dc2589c830"],"layout":"IPY_MODEL_8f1dadc3e2e14feebdd92613c93808d5"}},"e0c50ee4f3d64230b95d28b3a384cfdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cc3de48101c48afa99313eb465f44f4","placeholder":"​","style":"IPY_MODEL_4197ee3f7de64579add004cadfb1b80f","value":"pytorch_model.bin: 100%"}},"37822fdbd8eb405892b2a6b5e780e304":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf46dc4e470a487d8771f6303545fc28","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7e11b2004ef413bb9cc860b83504a2f","value":442221694}},"ce926a73a41c4262957ec1dc2589c830":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77430f85ae2f4503a3465110c3516ca7","placeholder":"​","style":"IPY_MODEL_352bd1f9eaae46a2b0190019c599f1a1","value":" 442M/442M [00:02&lt;00:00, 205MB/s]"}},"8f1dadc3e2e14feebdd92613c93808d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cc3de48101c48afa99313eb465f44f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4197ee3f7de64579add004cadfb1b80f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf46dc4e470a487d8771f6303545fc28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7e11b2004ef413bb9cc860b83504a2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77430f85ae2f4503a3465110c3516ca7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"352bd1f9eaae46a2b0190019c599f1a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grjQLrTxL10Y","executionInfo":{"status":"ok","timestamp":1702159097774,"user_tz":300,"elapsed":18092,"user":{"displayName":"paleti krishnasai","userId":"01090988434910113765"}},"outputId":"0965d397-f5c5-42fa-d713-b1cd9cc59e64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["## The following Cell will take approximately 1hour to install all packages required."],"metadata":{"id":"V9e2So1lI-Gw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVnGSv6cxVKj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8563d12-0819-40c6-a7a5-618b934312ca","executionInfo":{"status":"ok","timestamp":1702162007509,"user_tz":300,"elapsed":2861311,"user":{"displayName":"paleti krishnasai","userId":"01090988434910113765"}}},"source":["# Install required packages.\n","!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-geometric\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/108.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}]},{"cell_type":"code","metadata":{"id":"_XgTpm9ZxoN9"},"source":["import os\n","import shutil\n","import time\n","\n","import math\n","\n","import csv\n","import random\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","import tokenizers\n","from tokenizers import Tokenizer\n","from transformers import BertTokenizerFast, BertModel\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hn1sOS313q5T"},"source":["import os.path as osp\n","import zipfile\n","\n","import torch\n","from torch_geometric.data import download_url, Data\n","from torch_geometric.data import Dataset as GeoDataset\n","from torch_geometric.data import DataLoader as GeoDataLoader\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gpao3ymyTBg"},"source":["#Need a special generator for random sampling:\n","\n","\n","class GenerateData():\n","  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n","    self.path_train = path_train\n","    self.path_val = path_val\n","    self.path_test = path_test\n","    self.path_molecules = path_molecules\n","    self.path_token_embs = path_token_embs\n","\n","    self.text_trunc_length = 256\n","\n","    self.prep_text_tokenizer()\n","\n","    self.load_substructures()\n","\n","    self.batch_size = 32\n","\n","    self.store_descriptions()\n","\n","  def load_substructures(self):\n","    self.molecule_sentences = {}\n","    self.molecule_tokens = {}\n","\n","    total_tokens = set()\n","    self.max_mol_length = 0\n","    with open(self.path_molecules) as f:\n","      for line in f:\n","        spl = line.split(\":\")\n","        cid = spl[0]\n","        tokens = spl[1].strip()\n","        self.molecule_sentences[cid] = tokens\n","        t = tokens.split()\n","        total_tokens.update(t)\n","        size = len(t)\n","        if size > self.max_mol_length: self.max_mol_length = size\n","\n","\n","    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n","\n","\n","\n","  def prep_text_tokenizer(self):\n","    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","\n","  def store_descriptions(self):\n","    self.descriptions = {}\n","\n","    self.mols = {}\n","\n","\n","\n","    self.training_cids = []\n","    #get training set cids...\n","    with open(self.path_train) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.training_cids.append(line['cid'])\n","\n","    self.validation_cids = []\n","    #get validation set cids...\n","    with open(self.path_val) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.validation_cids.append(line['cid'])\n","\n","    self.test_cids = []\n","    #get test set cids...\n","    with open(self.path_test) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.test_cids.append(line['cid'])\n","\n","  def generate_examples_train(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.training_cids)\n","\n","    for cid in self.training_cids:\n","      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n","                                        padding='max_length', return_tensors = 'np')\n","\n","      yield {\n","          'cid': cid,\n","          'input': {\n","              'text': {\n","                'input_ids': text_input['input_ids'].squeeze(),\n","                'attention_mask': text_input['attention_mask'].squeeze(),\n","              },\n","              'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","              },\n","          },\n","      }\n","\n","\n","  def generate_examples_val(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.validation_cids)\n","\n","    for cid in self.validation_cids:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n","                                         max_length=self.text_trunc_length, return_tensors = 'np')\n","\n","        mol_input = []\n","\n","        yield {\n","            'cid': cid,\n","            'input': {\n","                'text': {\n","                  'input_ids': text_input['input_ids'].squeeze(),\n","                  'attention_mask': text_input['attention_mask'].squeeze(),\n","                },\n","                'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","                }\n","            },\n","        }\n","\n","\n","  def generate_examples_test(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.test_cids)\n","\n","    for cid in self.test_cids:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n","                                         max_length=self.text_trunc_length, return_tensors = 'np')\n","\n","        mol_input = []\n","\n","        yield {\n","            'cid': cid,\n","            'input': {\n","                'text': {\n","                  'input_ids': text_input['input_ids'].squeeze(),\n","                  'attention_mask': text_input['attention_mask'].squeeze(),\n","                },\n","                'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","                }\n","            },\n","        }\n","\n","\n","mounted_path_token_embs = \"/content/drive/MyDrive/597Project/text2mol/data/token_embedding_dict.npy\"\n","mounted_path_train = \"/content/drive/MyDrive/597Project/text2mol/data/training.txt\"\n","mounted_path_val = \"/content/drive/MyDrive/597Project/text2mol/data/val.txt\"\n","mounted_path_test = \"/content/drive/MyDrive/597Project/text2mol/data/test.txt\"\n","mounted_path_molecules = \"/content/drive/MyDrive/597Project/text2mol/data/ChEBI_defintions_substructure_corpus.cp\"\n","gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zck-zGTa8JOv"},"source":["\n","\n","class Dataset(Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, gen, length):\n","      'Initialization'\n","\n","      self.gen = gen\n","      self.it = iter(self.gen())\n","\n","      self.length = length\n","\n","  def __len__(self):\n","      'Denotes the total number of samples'\n","      return self.length\n","\n","\n","  def __getitem__(self, index):\n","      'Generates one sample of data'\n","\n","      try:\n","        ex = next(self.it)\n","      except StopIteration:\n","        self.it = iter(self.gen())\n","        ex = next(self.it)\n","\n","      X = ex['input']\n","      y = 1\n","\n","      return X, y\n","\n","training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n","validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n","test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Fj8h8vhk3W0"},"source":["\n","# Parameters\n","params = {'batch_size': gt.batch_size,\n","          'shuffle': True,\n","          'num_workers': 1}\n","\n","training_generator = DataLoader(training_set, **params)\n","validation_generator = DataLoader(validation_set, **params)\n","test_generator = DataLoader(test_set, **params)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3q_Y5pNF3uxO"},"source":["\n","\n","class MoleculeGraphDataset(GeoDataset):\n","    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n","        self.cids = cids\n","        self.data_path = data_path\n","        self.gt = gt\n","        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n","\n","        self.idx_to_cid = {}\n","        i = 0\n","        for raw_path in self.raw_paths:\n","            cid = int(raw_path.split('/')[-1][:-6])\n","            self.idx_to_cid[i] = cid\n","            i += 1\n","\n","    @property\n","    def raw_file_names(self):\n","        return [cid + \".graph\" for cid in self.cids]\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['data_{}.pt'.format(cid) for cid in self.cids]\n","\n","    def download(self):\n","        # Download to `self.raw_dir`.\n","        shutil.copy(self.data_path, os.path.join(self.raw_dir, \"/mol_graphs.zip\"))\n","\n","    def process_graph(self, raw_path):\n","      edge_index  = []\n","      x = []\n","      with open(raw_path, 'r') as f:\n","        next(f)\n","        for line in f: #edges\n","          if line != \"\\n\":\n","            edge = *map(int, line.split()),\n","            edge_index.append(edge)\n","          else:\n","            break\n","        next(f)\n","        for line in f: #get mol2vec features:\n","          substruct_id = line.strip().split()[-1]\n","          if substruct_id in self.gt.token_embs:\n","            x.append(self.gt.token_embs[substruct_id])\n","          else:\n","            x.append(self.gt.token_embs['UNK'])\n","\n","        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n","\n","\n","\n","    def process(self):\n","        #self.processed_dir = \"/content/drive/MyDrive/597Project/text2mol/data/processed\"\n","        with zipfile.ZipFile(os.path.join(self.raw_dir, \"/mol_graphs.zip\"), 'r') as zip_ref:\n","            zip_ref.extractall(self.raw_dir)\n","\n","\n","        i = 0\n","        for raw_path in self.raw_paths:\n","            # Read data from `raw_path`.\n","\n","            cid = int(raw_path.split('/')[-1][:-6])\n","\n","            edge_index, x = self.process_graph(raw_path)\n","            data = Data(x=x, edge_index = edge_index)\n","\n","            if self.pre_filter is not None and not self.pre_filter(data):\n","                continue\n","\n","            if self.pre_transform is not None:\n","                data = self.pre_transform(data)\n","\n","            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n","            i += 1\n","\n","    def len(self):\n","        return len(self.processed_file_names)\n","\n","    def get(self, idx):\n","        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n","        return data\n","\n","    def get_cid(self, cid):\n","        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n","        return data\n","\n","#To get specific lists...\n","\n","class CustomGraphCollater(object):\n","    def __init__(self, dataset, follow_batch = [], exclude_keys = []):\n","        self.follow_batch = follow_batch\n","        self.exclude_keys = exclude_keys\n","        self.dataset = dataset\n","\n","    def collate(self, batch):\n","        elem = batch[0]\n","        if isinstance(elem, Data):\n","            return Batch.from_data_list(batch)\n","\n","        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n","\n","    def __call__(self, cids):\n","\n","        return self.collate([self.dataset.get_cid(int(cid)) for cid in cids])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEEojof83vRE"},"source":["root = '/content/drive/MyDrive/597Project/text2mol/data/exp'\n","graph_data_path = \"/content/drive/MyDrive/597Project/text2mol/data/mol_graphs.zip\"\n","\n","\n","mg_data_tr = MoleculeGraphDataset(root, gt.training_cids, graph_data_path, gt)\n","graph_batcher_tr = CustomGraphCollater(mg_data_tr)\n","\n","mg_data_val = MoleculeGraphDataset(root, gt.validation_cids, graph_data_path, gt)\n","graph_batcher_val = CustomGraphCollater(mg_data_val)\n","\n","mg_data_test = MoleculeGraphDataset(root, gt.test_cids, graph_data_path, gt)\n","graph_batcher_test = CustomGraphCollater(mg_data_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aksj743St9ga"},"source":["\n","class Model(nn.Module):\n","    def __init__(self, ntoken, ninp, nout, nhid, graph_hidden_channels, dropout=0.5):\n","        super(Model, self).__init__()\n","\n","\n","        self.text_hidden1 = nn.Linear(ninp, nout)\n","\n","        self.ninp = ninp\n","        self.nhid = nhid\n","        self.nout = nout\n","\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        self.temp = nn.Parameter(torch.Tensor([0.07]))\n","        self.register_parameter( 'temp' , self.temp )\n","\n","        self.ln1 = nn.LayerNorm((nout))\n","        self.ln2 = nn.LayerNorm((nout))\n","\n","        self.relu = nn.ReLU()\n","        self.selu = nn.SELU()\n","\n","        #For GCN:\n","        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n","        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n","        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n","        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n","        self.mol_hidden2 = nn.Linear(nhid, nhid)\n","        self.mol_hidden3 = nn.Linear(nhid, nout)\n","\n","\n","        self.other_params = list(self.parameters()) #get all but bert params\n","\n","        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n","        self.text_transformer_model.train()\n","\n","    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n","\n","        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n","\n","        text_x = text_encoder_output['pooler_output']\n","        text_x = self.text_hidden1(text_x)\n","\n","\n","        #Obtain node embeddings\n","        x = graph_batch.x\n","        edge_index = graph_batch.edge_index\n","        batch = graph_batch.batch\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        x = self.conv3(x, edge_index)\n","\n","        # Readout layer\n","        x = global_mean_pool(x, batch)  # [batch_size, graph_hidden_channels]\n","\n","\n","        x = self.mol_hidden1(x).relu()\n","        x = self.mol_hidden2(x).relu()\n","        x = self.mol_hidden3(x)\n","\n","\n","        x = self.ln1(x)\n","        text_x = self.ln2(text_x)\n","\n","        x = x * torch.exp(self.temp)\n","        text_x = text_x * torch.exp(self.temp)\n","\n","        return text_x, x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGMF8AZcB2Zy","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3da40b13e3034295b28e357e992ffd92","e0c50ee4f3d64230b95d28b3a384cfdf","37822fdbd8eb405892b2a6b5e780e304","ce926a73a41c4262957ec1dc2589c830","8f1dadc3e2e14feebdd92613c93808d5","5cc3de48101c48afa99313eb465f44f4","4197ee3f7de64579add004cadfb1b80f","cf46dc4e470a487d8771f6303545fc28","f7e11b2004ef413bb9cc860b83504a2f","77430f85ae2f4503a3465110c3516ca7","352bd1f9eaae46a2b0190019c599f1a1"]},"executionInfo":{"status":"ok","timestamp":1702169868451,"user_tz":300,"elapsed":4224,"user":{"displayName":"paleti krishnasai","userId":"01090988434910113765"}},"outputId":"5e6cea15-8c55-4a78-debc-58922040e957"},"source":["model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nhid = 600, nout = 300, graph_hidden_channels = 600)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da40b13e3034295b28e357e992ffd92"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"P9eP2y9dbw32"},"source":["import torch.optim as optim\n","from transformers.optimization import get_linear_schedule_with_warmup\n","\n","epochs = 5\n","\n","init_lr = 1e-4\n","bert_lr = 3e-5\n","bert_params = list(model.text_transformer_model.parameters())\n","\n","optimizer = optim.Adam([\n","                {'params': model.other_params},\n","                {'params': bert_params, 'lr': bert_lr}\n","            ], lr=init_lr)\n","\n","num_warmup_steps = 1000\n","num_training_steps = epochs * len(training_generator) - num_warmup_steps\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1heECu1nVRB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702169880030,"user_tz":300,"elapsed":6818,"user":{"displayName":"paleti krishnasai","userId":"01090988434910113765"}},"outputId":"64678f16-df21-4b64-afb4-80d26c6252fa"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)\n","\n","tmp = model.to(device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"HytSaAyHNBuZ"},"source":["criterion = nn.CrossEntropyLoss()\n","\n","def loss_func(v1, v2):\n","  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n","  labels = torch.arange(logits.shape[0]).to(device)\n","  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtfDFAnN_Neu"},"source":["train_losses = []\n","val_losses = []\n","\n","train_acc = []\n","val_acc = []\n","\n","mounted_path = \"INSERT PATH TO /GCN_outputs/\" #create a folder to store GCN outputs and provide the path here.\n","if not os.path.exists(mounted_path):\n","  os.makedirs(mounted_path)\n","\n","# Loop over epochs\n","for epoch in range(epochs):\n","    # Training\n","\n","    start_time = time.time()\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    model.train()\n","    for i, d in enumerate(training_generator):\n","        batch, labels = d\n","        # Transfer to GPU\n","\n","        text_mask = batch['text']['attention_mask'].bool()\n","\n","        text = batch['text']['input_ids'].to(device)\n","        text_mask = text_mask.to(device)\n","        graph_batch = graph_batcher_tr(d[0]['molecule']['cid']).to(device)\n","\n","\n","        text_out, chem_out = model(text, graph_batch, text_mask)\n","\n","        loss = loss_func(text_out, chem_out).to(device)\n","\n","        running_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        scheduler.step()\n","\n","        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","    train_losses.append(running_loss / (i+1))\n","    train_acc.append(running_acc / (i+1))\n","\n","    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","\n","\n","\n","    # Validation\n","    model.eval()\n","    with torch.set_grad_enabled(False):\n","      start_time = time.time()\n","      running_acc = 0.0\n","      running_loss = 0.0\n","      for i, d in enumerate(validation_generator):\n","          batch, labels = d\n","          # Transfer to GPU\n","\n","          text_mask = batch['text']['attention_mask'].bool()\n","\n","          text = batch['text']['input_ids'].to(device)\n","          text_mask = text_mask.to(device)\n","          graph_batch = graph_batcher_val(d[0]['molecule']['cid']).to(device)\n","\n","\n","          text_out, chem_out = model(text, graph_batch, text_mask)\n","\n","          loss = loss_func(text_out, chem_out).to(device)\n","          running_loss += loss.item()\n","\n","          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","      val_losses.append(running_loss / (i+1))\n","      val_acc.append(running_acc / (i+1))\n","\n","\n","      min_loss = np.min(val_losses)\n","      if val_losses[-1] == min_loss:\n","          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n","\n","    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","\n","\n","#Save last accuracy:\n","torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BzSbNJ4fWGN6"},"source":["## Extract Embeddings\n","\n"]},{"cell_type":"code","metadata":{"id":"oCP_DuaaWKtV"},"source":["\n","mounted_path = \"INSERT PATH TO text2mol/code/notebooks/GCN_outputs/\"\n","\n","cids_train = np.array([])\n","cids_val = np.array([])\n","cids_test = np.array([])\n","chem_embeddings_train = np.array([])\n","text_embeddings_train = np.array([])\n","chem_embeddings_val = np.array([])\n","text_embeddings_val = np.array([])\n","chem_embeddings_test = np.array([])\n","text_embeddings_test = np.array([])\n","\n","\n","with torch.no_grad():\n","  for i, d in enumerate(gt.generate_examples_train()):\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    graph_batch = graph_batcher_tr([d['input']['molecule']['cid']]).to(device)\n","    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","    text_emb, chem_emb = model(text, graph_batch, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n","    chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n","    text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n","\n","    if (i+1) % 1000 == 0: print(i+1, \"samples eval.\")\n","\n","\n","  print(cids_train.shape, chem_embeddings_train.shape)\n","\n","  for d in gt.generate_examples_val():\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    graph_batch = graph_batcher_val([d['input']['molecule']['cid']]).to(device)\n","    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","\n","    text_emb, chem_emb = model(text, graph_batch, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n","    chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n","    text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n","\n","  print(cids_val.shape, chem_embeddings_val.shape)\n","\n","  for d in gt.generate_examples_test():\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    graph_batch = graph_batcher_test([d['input']['molecule']['cid']]).to(device)\n","    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","\n","    text_emb, chem_emb = model(text, graph_batch, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n","    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n","    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n","\n","print(cids_test.shape, chem_embeddings_test.shape)\n","\n","emb_path = \"INSERT PATH TO /text2mol/code/notebooks/GCN_Embeddings/\"\n","if not os.path.exists(emb_path):\n","  os.mkdir(emb_path)\n","np.save(emb_path+\"cids_train.npy\", cids_train)\n","np.save(emb_path+\"cids_val.npy\", cids_val)\n","np.save(emb_path+\"cids_test.npy\", cids_test)\n","np.save(emb_path+\"chem_embeddings_train.npy\", chem_embeddings_train)\n","np.save(emb_path+\"chem_embeddings_val.npy\", chem_embeddings_val)\n","np.save(emb_path+\"chem_embeddings_test.npy\", chem_embeddings_test)\n","np.save(emb_path+\"text_embeddings_train.npy\", text_embeddings_train)\n","np.save(emb_path+\"text_embeddings_val.npy\", text_embeddings_val)\n","np.save(emb_path+\"text_embeddings_test.npy\", text_embeddings_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PXxGk7sjO1u2"},"execution_count":null,"outputs":[]}]}