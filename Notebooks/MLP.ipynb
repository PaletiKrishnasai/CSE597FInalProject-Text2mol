{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ma9HeEci0Jaq","executionInfo":{"status":"ok","timestamp":1702096226191,"user_tz":300,"elapsed":1031,"user":{"displayName":"Krishnasai Paleti","userId":"16153431904125323197"}},"outputId":"2952cf8a-41a6-4860-c69d-78272a92b7f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"OVnGSv6cxVKj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702096231744,"user_tz":300,"elapsed":5556,"user":{"displayName":"Krishnasai Paleti","userId":"16153431904125323197"}},"outputId":"d1477d97-036e-4821-a3e8-6c9059f26a75"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}]},{"cell_type":"code","metadata":{"id":"_XgTpm9ZxoN9"},"source":["import os\n","import shutil\n","import time\n","\n","import math\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import tokenizers\n","from tokenizers import Tokenizer\n","from transformers import BertTokenizerFast, BertModel\n","\n","import csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gpao3ymyTBg"},"source":["#Need a special generator for random sampling:\n","\n","class GenerateData():\n","  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n","    self.path_train = path_train\n","    self.path_val = path_val\n","    self.path_test = path_test\n","    self.path_molecules = path_molecules\n","    self.path_token_embs = path_token_embs\n","\n","    self.text_trunc_length = 256\n","\n","    self.prep_text_tokenizer()\n","\n","    self.load_substructures()\n","\n","    self.batch_size = 32\n","\n","    self.store_descriptions()\n","\n","  def load_substructures(self):\n","    self.molecule_sentences = {}\n","    self.molecule_tokens = {}\n","\n","    total_tokens = set()\n","    self.max_mol_length = 0\n","    with open(self.path_molecules) as f:\n","      for line in f:\n","        spl = line.split(\":\")\n","        cid = spl[0]\n","        tokens = spl[1].strip()\n","        self.molecule_sentences[cid] = tokens\n","        t = tokens.split()\n","        total_tokens.update(t)\n","        size = len(t)\n","        if size > self.max_mol_length: self.max_mol_length = size\n","\n","\n","    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n","\n","\n","\n","  def prep_text_tokenizer(self):\n","    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","\n","  def store_descriptions(self):\n","    self.descriptions = {}\n","\n","    self.mols = {}\n","\n","\n","\n","    self.training_cids = []\n","    #get training set cids...\n","    with open(self.path_train) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.training_cids.append(line['cid'])\n","\n","    self.validation_cids = []\n","    #get validation set cids...\n","    with open(self.path_val) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.validation_cids.append(line['cid'])\n","\n","    self.test_cids = []\n","    #get test set cids...\n","    with open(self.path_test) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.test_cids.append(line['cid'])\n","\n","  def generate_examples_train(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.training_cids)\n","\n","    for cid in self.training_cids:\n","      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n","                                        padding='max_length', return_tensors = 'np')\n","\n","      yield {\n","          'cid': cid,\n","          'input': {\n","              'text': {\n","                'input_ids': text_input['input_ids'].squeeze(),\n","                'attention_mask': text_input['attention_mask'].squeeze(),\n","              },\n","              'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","              },\n","          },\n","      }\n","\n","\n","  def generate_examples_val(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.validation_cids)\n","\n","    for cid in self.validation_cids:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n","                                         max_length=self.text_trunc_length, return_tensors = 'np')\n","\n","        mol_input = []\n","\n","        yield {\n","            'cid': cid,\n","            'input': {\n","                'text': {\n","                  'input_ids': text_input['input_ids'].squeeze(),\n","                  'attention_mask': text_input['attention_mask'].squeeze(),\n","                },\n","                'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","                }\n","            },\n","        }\n","\n","\n","  def generate_examples_test(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.test_cids)\n","\n","    for cid in self.test_cids:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n","                                         max_length=self.text_trunc_length, return_tensors = 'np')\n","\n","        mol_input = []\n","\n","        yield {\n","            'cid': cid,\n","            'input': {\n","                'text': {\n","                  'input_ids': text_input['input_ids'].squeeze(),\n","                  'attention_mask': text_input['attention_mask'].squeeze(),\n","                },\n","                'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","                }\n","            },\n","        }\n","\n","\n","\n","mounted_path_token_embs = \"INSERT PATH TO /token_embedding_dict.npy\" # removed my drive path to maintain privacy, the path is a clickable link and the my drive was accessible through my folder.\n","mounted_path_train = \"INSERT PATH TO /training.txt\"\n","mounted_path_val = \"INSERT PATH TO /val.txt\"\n","mounted_path_test = \"INSERT PATH TO /test.txt\"\n","mounted_path_molecules = \"INSERT PATH TO /ChEBI_defintions_substructure_corpus.cp\"\n","gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zck-zGTa8JOv"},"source":["\n","\n","class Dataset(Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, gen, length):\n","      'Initialization'\n","\n","      self.gen = gen\n","      self.it = iter(self.gen())\n","\n","      self.length = length\n","\n","  def __len__(self):\n","      'Denotes the total number of samples'\n","      return self.length\n","\n","\n","  def __getitem__(self, index):\n","      'Generates one sample of data'\n","\n","      try:\n","        ex = next(self.it)\n","      except StopIteration:\n","        self.it = iter(self.gen())\n","        ex = next(self.it)\n","\n","      X = ex['input']\n","      y = 1\n","\n","      return X, y\n","\n","training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n","validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n","test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Fj8h8vhk3W0"},"source":["\n","# Parameters\n","params = {'batch_size': gt.batch_size,\n","          'shuffle': True,\n","          'num_workers': 1}\n","\n","training_generator = DataLoader(training_set, **params)\n","validation_generator = DataLoader(validation_set, **params)\n","test_generator = DataLoader(test_set, **params)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aksj743St9ga"},"source":["\n","class Model(nn.Module):\n","    def __init__(self, ntoken, ninp, nout, nhid, dropout=0.5):\n","        super(Model, self).__init__()\n","\n","\n","        self.text_hidden1 = nn.Linear(ninp, nout)\n","\n","        self.ninp = ninp\n","        self.nhid = nhid\n","        self.nout = nout\n","\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        self.mol_hidden1 = nn.Linear(nout, nhid)\n","        self.mol_hidden2 = nn.Linear(nhid, nhid)\n","        self.mol_hidden3 = nn.Linear(nhid, nout)\n","\n","\n","        self.temp = nn.Parameter(torch.Tensor([0.07]))\n","        self.register_parameter( 'temp' , self.temp )\n","\n","        self.ln1 = nn.LayerNorm((nout))\n","        self.ln2 = nn.LayerNorm((nout))\n","\n","        self.relu = nn.ReLU()\n","        self.selu = nn.SELU()\n","\n","        self.other_params = list(self.parameters()) #get all but bert params\n","\n","        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n","        self.text_transformer_model.train()\n","\n","    def forward(self, text, molecule, text_mask = None, molecule_mask = None):\n","\n","        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n","\n","        text_x = text_encoder_output['pooler_output']\n","        text_x = self.text_hidden1(text_x)\n","\n","        x = self.relu(self.mol_hidden1(molecule))\n","        x = self.relu(self.mol_hidden2(x))\n","        x = self.mol_hidden3(x)\n","\n","\n","        x = self.ln1(x)\n","        text_x = self.ln2(text_x)\n","\n","        x = x * torch.exp(self.temp)\n","        text_x = text_x * torch.exp(self.temp)\n","\n","        return text_x, x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGMF8AZcB2Zy"},"source":["model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nhid = 600, nout = 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9eP2y9dbw32"},"source":["import torch.optim as optim\n","from transformers.optimization import get_linear_schedule_with_warmup\n","\n","epochs = 5\n","\n","init_lr = 1e-4\n","bert_lr = 3e-5\n","bert_params = list(model.text_transformer_model.parameters())\n","\n","optimizer = optim.Adam([\n","                {'params': model.other_params},\n","                {'params': bert_params, 'lr': bert_lr}\n","            ], lr=init_lr)\n","\n","num_warmup_steps = 1000\n","num_training_steps = epochs * len(training_generator) - num_warmup_steps\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1heECu1nVRB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702096242006,"user_tz":300,"elapsed":190,"user":{"displayName":"Krishnasai Paleti","userId":"16153431904125323197"}},"outputId":"95764bd7-ed1d-4acc-f3cd-f98ad12906a0"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)\n","\n","tmp = model.to(device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","metadata":{"id":"HytSaAyHNBuZ"},"source":["criterion = nn.CrossEntropyLoss()\n","\n","def loss_func(v1, v2):\n","  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n","  labels = torch.arange(logits.shape[0]).to(device)\n","  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtfDFAnN_Neu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702101832273,"user_tz":300,"elapsed":5590270,"user":{"displayName":"Krishnasai Paleti","userId":"16153431904125323197"}},"outputId":"cdc5a178-1f32-4d75-8cb9-8bb4f7a10ea3"},"source":["train_losses = []\n","val_losses = []\n","\n","train_acc = []\n","val_acc = []\n","\n","mounted_path = #\"/content/drive/MyDrive/597Project/text2mol/code/notebooks/MLP_outputs/\"\n","if not os.path.exists(mounted_path):\n","  os.mkdir(mounted_path)\n","\n","# Loop over epochs\n","for epoch in range(epochs):\n","    # Training\n","\n","    start_time = time.time()\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    model.train()\n","    for i, d in enumerate(training_generator):\n","        batch, labels = d\n","        # Transfer to GPU\n","\n","        text_mask = batch['text']['attention_mask'].bool()\n","\n","        text = batch['text']['input_ids'].to(device)\n","        text_mask = text_mask.to(device)\n","        molecule = batch['molecule']['mol2vec'].float().to(device)\n","\n","        text_out, chem_out = model(text, molecule, text_mask)\n","\n","        loss = loss_func(text_out, chem_out).to(device)\n","\n","        running_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        scheduler.step()\n","\n","        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","    train_losses.append(running_loss / (i+1))\n","    train_acc.append(running_acc / (i+1))\n","\n","    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","\n","\n","\n","    # Validation\n","    model.eval()\n","    with torch.set_grad_enabled(False):\n","      start_time = time.time()\n","      running_acc = 0.0\n","      running_loss = 0.0\n","      for i, d in enumerate(validation_generator):\n","          batch, labels = d\n","          # Transfer to GPU\n","\n","          text_mask = batch['text']['attention_mask'].bool()\n","\n","          text = batch['text']['input_ids'].to(device)\n","          text_mask = text_mask.to(device)\n","          molecule = batch['molecule']['mol2vec'].float().to(device)\n","\n","\n","\n","          text_out, chem_out = model(text, molecule, text_mask)\n","\n","          loss = loss_func(text_out, chem_out).to(device)\n","          running_loss += loss.item()\n","\n","          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","      val_losses.append(running_loss / (i+1))\n","      val_acc.append(running_acc / (i+1))\n","\n","\n","      min_loss = np.min(val_losses)\n","      if val_losses[-1] == min_loss:\n","          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n","\n","    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","\n","\n","torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100 batches trained. Avg loss:\t 24.694877195358277 . Avg ms/step = 1303.7605285644531\n","200 batches trained. Avg loss:\t 17.51105790376663 . Avg ms/step = 1296.0667252540588\n","300 batches trained. Avg loss:\t 14.165976985295613 . Avg ms/step = 1294.6666359901428\n","400 batches trained. Avg loss:\t 12.050130556821824 . Avg ms/step = 1293.4216111898422\n","500 batches trained. Avg loss:\t 10.444689309120179 . Avg ms/step = 1292.7729592323303\n","600 batches trained. Avg loss:\t 9.226741205453873 . Avg ms/step = 1291.8908631801605\n","700 batches trained. Avg loss:\t 8.25707125374249 . Avg ms/step = 1292.0967197418213\n","800 batches trained. Avg loss:\t 7.4786241401731965 . Avg ms/step = 1291.6204312443733\n","Epoch 0 training loss:\t\t 7.2964847361348735 . Time = 1065.876991033554 seconds.\n","100 batches eval. Avg loss:\t 1.6702240228652954 . Avg ms/step = 486.79227113723755\n","Epoch 0 validation loss:\t 1.649912561934728 . Time = 51.68130803108215 seconds.\n","100 batches trained. Avg loss:\t 1.568030782341957 . Avg ms/step = 1288.9556169509888\n","200 batches trained. Avg loss:\t 1.445399442613125 . Avg ms/step = 1290.247677564621\n","300 batches trained. Avg loss:\t 1.3700885059436163 . Avg ms/step = 1289.229878584544\n","400 batches trained. Avg loss:\t 1.2942499156296252 . Avg ms/step = 1289.5208501815796\n","500 batches trained. Avg loss:\t 1.219223804116249 . Avg ms/step = 1289.109079837799\n","600 batches trained. Avg loss:\t 1.1698080994685491 . Avg ms/step = 1288.7219154834747\n","700 batches trained. Avg loss:\t 1.104848239336695 . Avg ms/step = 1288.994574206216\n","800 batches trained. Avg loss:\t 1.0591623884066939 . Avg ms/step = 1289.0303647518158\n","Epoch 1 training loss:\t\t 1.0443700613503595 . Time = 1063.57346701622 seconds.\n","100 batches eval. Avg loss:\t 0.6601729129254817 . Avg ms/step = 486.6935729980469\n","Epoch 1 validation loss:\t 0.6564864804590448 . Time = 52.413129806518555 seconds.\n","100 batches trained. Avg loss:\t 0.5115644539892673 . Avg ms/step = 1292.6477551460266\n","200 batches trained. Avg loss:\t 0.4855875937640667 . Avg ms/step = 1291.018809080124\n","300 batches trained. Avg loss:\t 0.47385147848476966 . Avg ms/step = 1290.8290942509968\n","400 batches trained. Avg loss:\t 0.46795045512728395 . Avg ms/step = 1290.8331179618835\n","500 batches trained. Avg loss:\t 0.45556645108014343 . Avg ms/step = 1290.495792388916\n","600 batches trained. Avg loss:\t 0.4481218531914055 . Avg ms/step = 1290.5292868614197\n","700 batches trained. Avg loss:\t 0.4372214900329709 . Avg ms/step = 1290.3244873455592\n","800 batches trained. Avg loss:\t 0.43275007475633176 . Avg ms/step = 1290.8061876893044\n","Epoch 2 training loss:\t\t 0.43155970894951445 . Time = 1065.176117658615 seconds.\n","100 batches eval. Avg loss:\t 0.3913375205919147 . Avg ms/step = 486.86021089553833\n","Epoch 2 validation loss:\t 0.3864899240792371 . Time = 53.82332253456116 seconds.\n","100 batches trained. Avg loss:\t 0.2963720775395632 . Avg ms/step = 1292.5420832633972\n","200 batches trained. Avg loss:\t 0.27894781408831476 . Avg ms/step = 1292.7520060539246\n","300 batches trained. Avg loss:\t 0.2782291260361671 . Avg ms/step = 1291.3812239964802\n","400 batches trained. Avg loss:\t 0.27200284746475517 . Avg ms/step = 1291.0629731416702\n","500 batches trained. Avg loss:\t 0.2637002923935652 . Avg ms/step = 1290.6067624092102\n","600 batches trained. Avg loss:\t 0.25529260575693724 . Avg ms/step = 1290.5237285296123\n","700 batches trained. Avg loss:\t 0.24798316034288811 . Avg ms/step = 1290.4688041550774\n","800 batches trained. Avg loss:\t 0.24383331757679116 . Avg ms/step = 1290.4035112261772\n","Epoch 3 training loss:\t\t 0.24187348843168818 . Time = 1064.9378526210785 seconds.\n","100 batches eval. Avg loss:\t 0.3188049721345305 . Avg ms/step = 487.23299503326416\n","Epoch 3 validation loss:\t 0.3106685036190566 . Time = 55.49730658531189 seconds.\n","100 batches trained. Avg loss:\t 0.19828805042430758 . Avg ms/step = 1293.4716510772705\n","200 batches trained. Avg loss:\t 0.18705368873663247 . Avg ms/step = 1292.7239155769348\n","300 batches trained. Avg loss:\t 0.19711146808539828 . Avg ms/step = 1291.869255701701\n","400 batches trained. Avg loss:\t 0.19895313187269495 . Avg ms/step = 1291.2844532728195\n","500 batches trained. Avg loss:\t 0.19890421762876212 . Avg ms/step = 1290.9055662155151\n","600 batches trained. Avg loss:\t 0.19968206006257486 . Avg ms/step = 1290.4198519388835\n","700 batches trained. Avg loss:\t 0.20151744026424628 . Avg ms/step = 1290.452822276524\n","800 batches trained. Avg loss:\t 0.20408391575096174 . Avg ms/step = 1290.6885600090027\n","Epoch 4 training loss:\t\t 0.20446474046038224 . Time = 1065.1468405723572 seconds.\n","100 batches eval. Avg loss:\t 0.3217769262567163 . Avg ms/step = 487.5113010406494\n","Epoch 4 validation loss:\t 0.32244704604292146 . Time = 50.35942983627319 seconds.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BzSbNJ4fWGN6"},"source":["## Extract Embeddings\n","\n"]},{"cell_type":"code","metadata":{"id":"oCP_DuaaWKtV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702102549866,"user_tz":300,"elapsed":717609,"user":{"displayName":"Krishnasai Paleti","userId":"16153431904125323197"}},"outputId":"6a16e012-43c4-4216-fdda-6b98edbfb9e0"},"source":["\n","mounted_path = \"/content/drive/MyDrive/597Project/text2mol/code/notebooks/MLP_outputs/\"\n","\n","cids_train = np.array([])\n","cids_val = np.array([])\n","cids_test = np.array([])\n","chem_embeddings_train = np.array([])\n","text_embeddings_train = np.array([])\n","chem_embeddings_val = np.array([])\n","text_embeddings_val = np.array([])\n","chem_embeddings_test = np.array([])\n","text_embeddings_test = np.array([])\n","\n","\n","\n","with torch.no_grad():\n","  for i, d in enumerate(gt.generate_examples_train()):\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n","    text_emb, chem_emb = model(text, molecule, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n","    chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n","    text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n","\n","    if (i+1) % 100 == 0: print(i+1, \"samples eval.\")\n","\n","\n","  print(cids_train.shape, chem_embeddings_train.shape)\n","\n","  for d in gt.generate_examples_val():\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n","    text_emb, chem_emb = model(text, molecule, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n","    chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n","    text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n","\n","  print(cids_val.shape, chem_embeddings_val.shape)\n","\n","  for d in gt.generate_examples_test():\n","    cid = np.array([d['cid']])\n","    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n","    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n","    text_emb, chem_emb = model(text, molecule, text_mask)\n","\n","    chem_emb = chem_emb.cpu().numpy()\n","    text_emb = text_emb.cpu().numpy()\n","\n","    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n","    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n","    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n","\n","print(cids_test.shape, chem_embeddings_test.shape)\n","\n","emb_path = \"/content/drive/MyDrive/597Project/text2mol/code/notebooks/MLP_Embeddings/\"\n","if not os.path.exists(emb_path):\n","  os.mkdir(emb_path)\n","np.save(emb_path+\"cids_train.npy\", cids_train)\n","np.save(emb_path+\"cids_val.npy\", cids_val)\n","np.save(emb_path+\"cids_test.npy\", cids_test)\n","np.save(emb_path+\"chem_embeddings_train.npy\", chem_embeddings_train)\n","np.save(emb_path+\"chem_embeddings_val.npy\", chem_embeddings_val)\n","np.save(emb_path+\"chem_embeddings_test.npy\", chem_embeddings_test)\n","np.save(emb_path+\"text_embeddings_train.npy\", text_embeddings_train)\n","np.save(emb_path+\"text_embeddings_val.npy\", text_embeddings_val)\n","np.save(emb_path+\"text_embeddings_test.npy\", text_embeddings_test)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100 samples eval.\n","200 samples eval.\n","300 samples eval.\n","400 samples eval.\n","500 samples eval.\n","600 samples eval.\n","700 samples eval.\n","800 samples eval.\n","900 samples eval.\n","1000 samples eval.\n","1100 samples eval.\n","1200 samples eval.\n","1300 samples eval.\n","1400 samples eval.\n","1500 samples eval.\n","1600 samples eval.\n","1700 samples eval.\n","1800 samples eval.\n","1900 samples eval.\n","2000 samples eval.\n","2100 samples eval.\n","2200 samples eval.\n","2300 samples eval.\n","2400 samples eval.\n","2500 samples eval.\n","2600 samples eval.\n","2700 samples eval.\n","2800 samples eval.\n","2900 samples eval.\n","3000 samples eval.\n","3100 samples eval.\n","3200 samples eval.\n","3300 samples eval.\n","3400 samples eval.\n","3500 samples eval.\n","3600 samples eval.\n","3700 samples eval.\n","3800 samples eval.\n","3900 samples eval.\n","4000 samples eval.\n","4100 samples eval.\n","4200 samples eval.\n","4300 samples eval.\n","4400 samples eval.\n","4500 samples eval.\n","4600 samples eval.\n","4700 samples eval.\n","4800 samples eval.\n","4900 samples eval.\n","5000 samples eval.\n","5100 samples eval.\n","5200 samples eval.\n","5300 samples eval.\n","5400 samples eval.\n","5500 samples eval.\n","5600 samples eval.\n","5700 samples eval.\n","5800 samples eval.\n","5900 samples eval.\n","6000 samples eval.\n","6100 samples eval.\n","6200 samples eval.\n","6300 samples eval.\n","6400 samples eval.\n","6500 samples eval.\n","6600 samples eval.\n","6700 samples eval.\n","6800 samples eval.\n","6900 samples eval.\n","7000 samples eval.\n","7100 samples eval.\n","7200 samples eval.\n","7300 samples eval.\n","7400 samples eval.\n","7500 samples eval.\n","7600 samples eval.\n","7700 samples eval.\n","7800 samples eval.\n","7900 samples eval.\n","8000 samples eval.\n","8100 samples eval.\n","8200 samples eval.\n","8300 samples eval.\n","8400 samples eval.\n","8500 samples eval.\n","8600 samples eval.\n","8700 samples eval.\n","8800 samples eval.\n","8900 samples eval.\n","9000 samples eval.\n","9100 samples eval.\n","9200 samples eval.\n","9300 samples eval.\n","9400 samples eval.\n","9500 samples eval.\n","9600 samples eval.\n","9700 samples eval.\n","9800 samples eval.\n","9900 samples eval.\n","10000 samples eval.\n","10100 samples eval.\n","10200 samples eval.\n","10300 samples eval.\n","10400 samples eval.\n","10500 samples eval.\n","10600 samples eval.\n","10700 samples eval.\n","10800 samples eval.\n","10900 samples eval.\n","11000 samples eval.\n","11100 samples eval.\n","11200 samples eval.\n","11300 samples eval.\n","11400 samples eval.\n","11500 samples eval.\n","11600 samples eval.\n","11700 samples eval.\n","11800 samples eval.\n","11900 samples eval.\n","12000 samples eval.\n","12100 samples eval.\n","12200 samples eval.\n","12300 samples eval.\n","12400 samples eval.\n","12500 samples eval.\n","12600 samples eval.\n","12700 samples eval.\n","12800 samples eval.\n","12900 samples eval.\n","13000 samples eval.\n","13100 samples eval.\n","13200 samples eval.\n","13300 samples eval.\n","13400 samples eval.\n","13500 samples eval.\n","13600 samples eval.\n","13700 samples eval.\n","13800 samples eval.\n","13900 samples eval.\n","14000 samples eval.\n","14100 samples eval.\n","14200 samples eval.\n","14300 samples eval.\n","14400 samples eval.\n","14500 samples eval.\n","14600 samples eval.\n","14700 samples eval.\n","14800 samples eval.\n","14900 samples eval.\n","15000 samples eval.\n","15100 samples eval.\n","15200 samples eval.\n","15300 samples eval.\n","15400 samples eval.\n","15500 samples eval.\n","15600 samples eval.\n","15700 samples eval.\n","15800 samples eval.\n","15900 samples eval.\n","16000 samples eval.\n","16100 samples eval.\n","16200 samples eval.\n","16300 samples eval.\n","16400 samples eval.\n","16500 samples eval.\n","16600 samples eval.\n","16700 samples eval.\n","16800 samples eval.\n","16900 samples eval.\n","17000 samples eval.\n","17100 samples eval.\n","17200 samples eval.\n","17300 samples eval.\n","17400 samples eval.\n","17500 samples eval.\n","17600 samples eval.\n","17700 samples eval.\n","17800 samples eval.\n","17900 samples eval.\n","18000 samples eval.\n","18100 samples eval.\n","18200 samples eval.\n","18300 samples eval.\n","18400 samples eval.\n","18500 samples eval.\n","18600 samples eval.\n","18700 samples eval.\n","18800 samples eval.\n","18900 samples eval.\n","19000 samples eval.\n","19100 samples eval.\n","19200 samples eval.\n","19300 samples eval.\n","19400 samples eval.\n","19500 samples eval.\n","19600 samples eval.\n","19700 samples eval.\n","19800 samples eval.\n","19900 samples eval.\n","20000 samples eval.\n","20100 samples eval.\n","20200 samples eval.\n","20300 samples eval.\n","20400 samples eval.\n","20500 samples eval.\n","20600 samples eval.\n","20700 samples eval.\n","20800 samples eval.\n","20900 samples eval.\n","21000 samples eval.\n","21100 samples eval.\n","21200 samples eval.\n","21300 samples eval.\n","21400 samples eval.\n","21500 samples eval.\n","21600 samples eval.\n","21700 samples eval.\n","21800 samples eval.\n","21900 samples eval.\n","22000 samples eval.\n","22100 samples eval.\n","22200 samples eval.\n","22300 samples eval.\n","22400 samples eval.\n","22500 samples eval.\n","22600 samples eval.\n","22700 samples eval.\n","22800 samples eval.\n","22900 samples eval.\n","23000 samples eval.\n","23100 samples eval.\n","23200 samples eval.\n","23300 samples eval.\n","23400 samples eval.\n","23500 samples eval.\n","23600 samples eval.\n","23700 samples eval.\n","23800 samples eval.\n","23900 samples eval.\n","24000 samples eval.\n","24100 samples eval.\n","24200 samples eval.\n","24300 samples eval.\n","24400 samples eval.\n","24500 samples eval.\n","24600 samples eval.\n","24700 samples eval.\n","24800 samples eval.\n","24900 samples eval.\n","25000 samples eval.\n","25100 samples eval.\n","25200 samples eval.\n","25300 samples eval.\n","25400 samples eval.\n","25500 samples eval.\n","25600 samples eval.\n","25700 samples eval.\n","25800 samples eval.\n","25900 samples eval.\n","26000 samples eval.\n","26100 samples eval.\n","26200 samples eval.\n","26300 samples eval.\n","26400 samples eval.\n","(26408,) (26408, 300)\n","(3301,) (3301, 300)\n","(3301,) (3301, 300)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0O07bLE76dFA"},"execution_count":null,"outputs":[]}]}