{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVHmu8OnlR9k","executionInfo":{"status":"ok","timestamp":1702176906882,"user_tz":300,"elapsed":1108,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"f0ebec97-95d9-4ed4-996c-cb2541416251"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["## The following Cell will take approximately 1hour to install all packages required."],"metadata":{"id":"WhgfK5p9JGuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVnGSv6cxVKj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702179689805,"user_tz":300,"elapsed":2782928,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"c1c300c4-30aa-43a9-d098-54f2aee93a86"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["# Install required packages.\n","!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n","!pip install -q torch-geometric\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XgTpm9ZxoN9"},"outputs":[],"source":["import os\n","import shutil\n","import time\n","\n","import math\n","\n","import csv\n","import random\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import Transformer\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n","\n","\n","import tokenizers\n","from tokenizers import Tokenizer\n","from transformers import BertTokenizerFast, BertModel\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaoYuRw0o0W8"},"outputs":[],"source":["import os.path as osp\n","import zipfile\n","\n","import torch\n","from torch_geometric.data import download_url, Data\n","from torch_geometric.data import Dataset as GeoDataset\n","from torch_geometric.data import DataLoader as GeoDataLoader\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gpao3ymyTBg","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["7a4173cbdf3c4951bee97344a9df41bb","f99cac77cffb492a9b1b4f922413e022","617bce91a742494eaefc97565046f254","2c390503a53c4d859fc9eff9a0f25c85","6a1c22c9b0364337b963106c0809e82b","a7385d5a059d4fc9a624e644eb88c9fb","5ae2572873754f14ac68168315dea5c7","ceb5ff331396400493a8b9b8d3332435","1657dded2b1b4a62947a742f11c1c324","eb35a69b659645a7a53898c7191dc389","db4227adec5d408286e54caf72655e79","263f594fcac64ba298e9f2232e411b8f","1d83701e239f4a9b9fec674f12b27480","3c04d50cd546454296bdffe43e28e48b","20585ed0a9e845ba832098d5b403431e","4b7f8ccf48eb4f9aad034d0705b8f247","d503127fad7a4b809d87de27eef1b757","7dccc5e7dd1745dc885047fa65ca0c28","06bce96e63894f3492e70308532dbd77","9bc1149e590344048d5a9488109d0e18","478d9bb35b2c4839b82a37457ebf1dc4","107592c9fc1b444fa03c01395c7f4900"]},"executionInfo":{"status":"ok","timestamp":1702179711593,"user_tz":300,"elapsed":11941,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"5f87c140-c05e-4354-9f51-14f5d6cfff47"},"outputs":[{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4173cbdf3c4951bee97344a9df41bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263f594fcac64ba298e9f2232e411b8f"}},"metadata":{}}],"source":["#Need a special generator for random sampling:\n","\n","\n","class GenerateData():\n","  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n","    self.path_train = path_train\n","    self.path_val = path_val\n","    self.path_test = path_test\n","    self.path_molecules = path_molecules\n","    self.path_token_embs = path_token_embs\n","\n","    self.mol_trunc_length = 512\n","    self.text_trunc_length = 256\n","\n","    self.prep_text_tokenizer()\n","\n","    self.load_substructures()\n","\n","    self.batch_size = 32\n","\n","    self.store_descriptions()\n","\n","  def load_substructures(self):\n","    self.molecule_sentences = {}\n","    self.molecule_tokens = {}\n","\n","    total_tokens = set()\n","    self.max_mol_length = 0\n","    with open(self.path_molecules) as f:\n","      for line in f:\n","        spl = line.split(\":\")\n","        cid = spl[0]\n","        tokens = spl[1].strip()\n","        self.molecule_sentences[cid] = tokens\n","        t = tokens.split()\n","        total_tokens.update(t)\n","        size = len(t)\n","        if size > self.max_mol_length: self.max_mol_length = size\n","\n","\n","    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n","\n","\n","\n","  def prep_text_tokenizer(self):\n","    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","\n","  def store_descriptions(self):\n","    self.descriptions = {}\n","\n","    self.mols = {}\n","\n","\n","\n","    self.training_cids = []\n","    #get training set cids...\n","    with open(self.path_train) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.training_cids.append(line['cid'])\n","\n","    self.validation_cids = []\n","    #get validation set cids...\n","    with open(self.path_val) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.validation_cids.append(line['cid'])\n","\n","    self.test_cids = []\n","    with open(self.path_test) as f:\n","      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n","      for n, line in enumerate(reader):\n","        self.descriptions[line['cid']] = line['desc']\n","        self.mols[line['cid']] = line['mol2vec']\n","        self.test_cids.append(line['cid'])\n","\n","  #transformers can't take array with full attention so have to pad a 0...\n","  def padarray(self, A, size, value=0):\n","      t = size - len(A)\n","      return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n","\n","\n","  def generate_examples_train(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.training_cids)\n","\n","    for cid in self.training_cids:\n","      label = np.random.randint(2)\n","      rand_cid = np.random.choice(self.training_cids)\n","      if label:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","      else:\n","        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","\n","      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n","      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n","\n","      yield {\n","          'cid': cid,\n","          'input': {\n","              'text': {\n","                'input_ids': text_ids,\n","                'attention_mask': text_mask,\n","              },\n","              'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","              },\n","          },\n","          'label': label\n","      }\n","\n","\n","  def generate_examples_val(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.validation_cids)\n","\n","    for cid in self.validation_cids:\n","      label = np.random.randint(2)\n","      rand_cid = np.random.choice(self.validation_cids)\n","      if label:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","      else:\n","        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","\n","\n","      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n","      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n","\n","      yield {\n","          'cid': cid,\n","          'input': {\n","              'text': {\n","                'input_ids': text_ids,\n","                'attention_mask': text_mask,\n","              },\n","              'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","              },\n","          },\n","          'label': label\n","      }\n","\n","  def generate_examples_test(self):\n","    \"\"\"Yields examples.\"\"\"\n","\n","    np.random.shuffle(self.test_cids)\n","\n","    for cid in self.test_cids:\n","      label = np.random.randint(2)\n","      rand_cid = np.random.choice(self.test_cids)\n","      if label:\n","        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","      else:\n","        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n","                                        padding='max_length', return_tensors = 'np')\n","\n","\n","      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n","      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n","\n","      yield {\n","          'cid': cid,\n","          'input': {\n","              'text': {\n","                'input_ids': text_ids,\n","                'attention_mask': text_mask,\n","              },\n","              'molecule' : {\n","                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n","                    'cid' : cid\n","              },\n","          },\n","          'label': label\n","      }\n","\n","\n","\n","mounted_path_token_embs = \"INSERT PATH TO /token_embedding_dict.npy\"\n","mounted_path_train = \"INSERT PATH TO /training.txt\"\n","mounted_path_val = \"INSERT PATH TO /val.txt\"\n","mounted_path_test = \"INSERT PATH TO /test.txt\"\n","mounted_path_molecules = \"INSERT PATH TO /ChEBI_defintions_substructure_corpus.cp\"\n","gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zck-zGTa8JOv"},"outputs":[],"source":["\n","\n","class Dataset(Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, gen, length):\n","      'Initialization'\n","\n","      self.gen = gen\n","      self.it = iter(self.gen())\n","\n","      self.length = length\n","\n","  def __len__(self):\n","      'Denotes the total number of samples'\n","      return self.length\n","\n","\n","  def __getitem__(self, index):\n","      'Generates one sample of data'\n","\n","      try:\n","        ex = next(self.it)\n","      except StopIteration:\n","        self.it = iter(self.gen())\n","        ex = next(self.it)\n","\n","      X = ex['input']\n","      y = ex['label']\n","\n","      return X, y\n","\n","training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n","validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n","test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Fj8h8vhk3W0"},"outputs":[],"source":["\n","# Parameters\n","params = {'batch_size': gt.batch_size,\n","          'shuffle': True,\n","          'num_workers': 1}\n","\n","training_generator = DataLoader(training_set, **params)\n","validation_generator = DataLoader(validation_set, **params)\n","test_generator = DataLoader(test_set, **params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MVm3jz5pIcf"},"outputs":[],"source":["\n","class MoleculeGraphDataset(GeoDataset):\n","    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n","        self.cids = cids\n","        self.data_path = data_path\n","        self.gt = gt\n","        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n","\n","        self.idx_to_cid = {}\n","        i = 0\n","        for raw_path in self.raw_paths:\n","            cid = int(raw_path.split('/')[-1][:-6])\n","            self.idx_to_cid[i] = cid\n","            i += 1\n","\n","    @property\n","    def raw_file_names(self):\n","        return [cid + \".graph\" for cid in self.cids]\n","\n","    @property\n","    def processed_file_names(self):\n","        return ['data_{}.pt'.format(cid) for cid in self.cids]\n","\n","    def download(self):\n","        # Download to `self.raw_dir`.\n","        shutil.copy(self.data_path, os.path.join(self.raw_dir, \"/mol_graphs.zip\"))\n","\n","    def process_graph(self, raw_path):\n","      edge_index  = []\n","      x = []\n","      with open(raw_path, 'r') as f:\n","        next(f)\n","        for line in f: #edges\n","          if line != \"\\n\":\n","            edge = *map(int, line.split()),\n","            edge_index.append(edge)\n","          else:\n","            break\n","        next(f)\n","        for line in f: #get mol2vec features:\n","          substruct_id = line.strip().split()[-1]\n","          if substruct_id in self.gt.token_embs:\n","            x.append(self.gt.token_embs[substruct_id])\n","          else:\n","            x.append(self.gt.token_embs['UNK'])\n","\n","        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n","\n","\n","\n","    def process(self):\n","\n","        with zipfile.ZipFile(os.path.join(self.raw_dir, \"/mol_graphs.zip\"), 'r') as zip_ref:\n","            zip_ref.extractall(self.raw_dir)\n","\n","\n","        i = 0\n","        for raw_path in self.raw_paths:\n","            # Read data from `raw_path`.\n","\n","            cid = int(raw_path.split('/')[-1][:-6])\n","\n","            edge_index, x = self.process_graph(raw_path)\n","            data = Data(x=x, edge_index = edge_index)\n","\n","            if self.pre_filter is not None and not self.pre_filter(data):\n","                continue\n","\n","            if self.pre_transform is not None:\n","                data = self.pre_transform(data)\n","\n","            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n","            i += 1\n","\n","    def len(self):\n","        return len(self.processed_file_names)\n","\n","    def get(self, idx):\n","        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n","        return data\n","\n","    def get_cid(self, cid):\n","        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n","        return data\n","\n","#To get specific lists...\n","\n","class CustomGraphCollater(object):\n","    def __init__(self, dataset, mask_len, follow_batch = [], exclude_keys = []):\n","        self.follow_batch = follow_batch\n","        self.exclude_keys = exclude_keys\n","        self.dataset = dataset\n","        self.mask_len = mask_len\n","        self.mask_indices = np.array(range(mask_len))\n","\n","    def generate_mask(self, sz):\n","        rv = torch.zeros((self.mask_len), dtype = torch.bool)\n","        rv = rv.masked_fill(torch.BoolTensor(self.mask_indices < sz), bool(1)) #pytorch transformer input version\n","        rv[-1] = 0 #set last value to 0 because pytorch can't handle all 1s\n","        return rv\n","\n","    def get_masks(self, batch):\n","      return torch.stack([self.generate_mask(b.x.shape[0]) for b in batch])\n","\n","    def collate(self, batch):\n","        elem = batch[0]\n","        if isinstance(elem, Data):\n","            return Batch.from_data_list(batch)\n","\n","        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n","\n","    def __call__(self, cids):\n","\n","        tmp = [self.dataset.get_cid(int(cid)) for cid in cids]\n","        return self.collate(tmp), self.get_masks(tmp)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkfbSCMipLbW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702181657668,"user_tz":300,"elapsed":1945883,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"7ac3e0d1-d0e9-4453-9a2c-72e747e437f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing...\n","<ipython-input-9-b68aaf8e2ade>:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n","<ipython-input-9-b68aaf8e2ade>:46: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n","  return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n","Done!\n","Processing...\n","Done!\n","Processing...\n","Done!\n"]}],"source":["root = '/content/drive/MyDrive/597Project/text2mol/data/root/'\n","graph_data_path = \"/content/drive/MyDrive/597Project/text2mol/data/mol_graphs.zip\"\n","\n","\n","mg_data_tr = MoleculeGraphDataset(root, gt.training_cids, graph_data_path, gt)\n","graph_batcher_tr = CustomGraphCollater(mg_data_tr, gt.mol_trunc_length)\n","\n","mg_data_val = MoleculeGraphDataset(root, gt.validation_cids, graph_data_path, gt)\n","graph_batcher_val = CustomGraphCollater(mg_data_val, gt.mol_trunc_length)\n","\n","mg_data_test = MoleculeGraphDataset(root, gt.test_cids, graph_data_path, gt)\n","graph_batcher_test = CustomGraphCollater(mg_data_test, gt.mol_trunc_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aksj743St9ga"},"outputs":[],"source":["\n","class Model(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nout, nhid, nhead, nlayers, graph_hidden_channels, mol_trunc_length,  dropout=0.5):\n","        super(Model, self).__init__()\n","\n","        self.text_hidden1 = nn.Linear(ninp, nhid)\n","        self.text_hidden2 = nn.Linear(nhid, nout)\n","\n","        self.ninp = ninp\n","        self.nhid = nhid\n","        self.nout = nout\n","\n","        self.graph_hidden_channels = graph_hidden_channels\n","\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n","        self.text_transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n","\n","\n","        self.temp = nn.Parameter(torch.Tensor([0.07]))\n","        self.register_parameter( 'temp' , self.temp )\n","\n","        self.ln1 = nn.LayerNorm((nout))\n","        self.ln2 = nn.LayerNorm((nout))\n","\n","        self.relu = nn.ReLU()\n","        self.selu = nn.SELU()\n","\n","        #For GCN:\n","        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n","        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n","        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n","        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n","        self.mol_hidden2 = nn.Linear(nhid, nout)\n","\n","\n","        self.other_params = list(self.parameters()) #get all but bert params\n","\n","        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n","        self.text_transformer_model.train()\n","\n","        self.device = 'cuda:0'\n","\n","    def set_device(self, dev):\n","        self.to(dev)\n","        self.device = dev\n","\n","    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n","\n","        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n","\n","        #Obtain node embeddings\n","        x = graph_batch.x\n","        edge_index = graph_batch.edge_index\n","        batch = graph_batch.batch\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        mol_x = self.conv3(x, edge_index)\n","\n","        #turn pytorch geometric output into the correct format for transformer\n","        #requires recovering the nodes from each graph into a separate dimension\n","        node_features = torch.zeros((graph_batch.num_graphs, gt.mol_trunc_length, self.graph_hidden_channels)).to(self.device)\n","        for i, p in enumerate(graph_batch.ptr):\n","          if p == 0:\n","            old_p = p\n","            continue\n","          node_features[i - 1, :p-old_p, :] = mol_x[old_p:torch.min(p, old_p + gt.mol_trunc_length), :]\n","          old_p = p\n","        node_features = torch.transpose(node_features, 0, 1)\n","\n","        text_output = self.text_transformer_decoder(text_encoder_output['last_hidden_state'].transpose(0,1), node_features,\n","                                                            tgt_key_padding_mask = text_mask == 0, memory_key_padding_mask = ~molecule_mask)\n","\n","\n","        #Readout layer\n","        x = global_mean_pool(mol_x, batch)  # [batch_size, graph_hidden_channels]\n","\n","        x = self.mol_hidden1(x)\n","        x = x.relu()\n","        x = self.mol_hidden2(x)\n","\n","        text_x = torch.tanh(self.text_hidden1(text_output[0,:,:])) #[CLS] pooler\n","        text_x = self.text_hidden2(text_x)\n","\n","        x = self.ln1(x)\n","        text_x = self.ln2(text_x)\n","\n","        x = x * torch.exp(self.temp)\n","        text_x = text_x * torch.exp(self.temp)\n","\n","        return text_x, x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGMF8AZcB2Zy"},"outputs":[],"source":["model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nout = 300, nhead = 8, nhid = 512, nlayers = 3, graph_hidden_channels = 768, mol_trunc_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9eP2y9dbw32"},"outputs":[],"source":["import torch.optim as optim\n","from transformers.optimization import get_linear_schedule_with_warmup\n","\n","epochs = 2\n","init_lr = 1e-4\n","bert_lr = 3e-5\n","bert_params = list(model.text_transformer_model.parameters())\n","\n","optimizer = optim.Adam([\n","                {'params': model.other_params},\n","                {'params': bert_params, 'lr': bert_lr}\n","            ], lr=init_lr)\n","\n","num_warmup_steps = 1000\n","num_training_steps = epochs * len(training_generator) - num_warmup_steps\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1heECu1nVRB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702185465145,"user_tz":300,"elapsed":184,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"010ebc22-5feb-41b6-f76f-d9860e7ec492"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(device)\n","\n","tmp = model.set_device(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HytSaAyHNBuZ"},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","def loss_func(v1, v2, labels):\n","  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n","  eye = torch.diag_embed(labels).to(device)\n","  return criterion(logits, eye) + criterion(torch.transpose(logits, 0, 1), eye), logits.diag() > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtfDFAnN_Neu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702188566518,"user_tz":300,"elapsed":3098149,"user":{"displayName":"kxp5619","userId":"17264164635712868718"}},"outputId":"d88fc8ee-b906-4e75-c9ae-45dc5e49c867"},"outputs":[{"output_type":"stream","name":"stdout","text":["100 batches trained. Avg loss:\t 5.38163197517395 Acc: 0.4946875 . Avg ms/step = 1724.3478465080261\n","200 batches trained. Avg loss:\t 3.136426603645086 Acc: 0.5003125 . Avg ms/step = 1754.644364118576\n","300 batches trained. Avg loss:\t 2.3451561067501703 Acc: 0.49364583333333334 . Avg ms/step = 1767.0649973551433\n","400 batches trained. Avg loss:\t 1.894480098411441 Acc: 0.494296875 . Avg ms/step = 1775.199527144432\n","500 batches trained. Avg loss:\t 1.594905375957489 Acc: 0.496625 . Avg ms/step = 1778.9749054908752\n","600 batches trained. Avg loss:\t 1.3784747965385515 Acc: 0.49921875 . Avg ms/step = 1781.8540116151173\n","700 batches trained. Avg loss:\t 1.2178183556454523 Acc: 0.4991964285714286 . Avg ms/step = 1783.7948107719421\n","800 batches trained. Avg loss:\t 1.0922376981936395 Acc: 0.500078125 . Avg ms/step = 1785.1386347413063\n","Epoch 0 training loss:\t\t 1.0646361027986317 . Time = 1473.327389240265 seconds.\n","Training accuracy: 0.49992433414043586\n","100 batches eval. Avg loss:\t 0.6440355595946312 . Avg ms/step = 677.4186825752258\n","Epoch 0 validation loss:\t 0.641390631118646 . Time = 73.42991590499878 seconds.\n","Validation accuracy: 0.6254807692307692\n","100 batches trained. Avg loss:\t 0.3116704773902893 Acc: 0.50375 . Avg ms/step = 1794.689860343933\n","200 batches trained. Avg loss:\t 0.23845666207373142 Acc: 0.50828125 . Avg ms/step = 1792.707313299179\n","300 batches trained. Avg loss:\t 0.21129336004455884 Acc: 0.5085416666666667 . Avg ms/step = 1791.5539741516113\n","400 batches trained. Avg loss:\t 0.1985279237665236 Acc: 0.506484375 . Avg ms/step = 1789.7835195064545\n","500 batches trained. Avg loss:\t 0.1910673746019602 Acc: 0.506625 . Avg ms/step = 1789.4500284194946\n","600 batches trained. Avg loss:\t 0.18557853425542514 Acc: 0.50625 . Avg ms/step = 1788.656837940216\n","700 batches trained. Avg loss:\t 0.18143220252224376 Acc: 0.5078125 . Avg ms/step = 1788.0133765084404\n","800 batches trained. Avg loss:\t 0.17856053007766604 Acc: 0.508359375 . Avg ms/step = 1787.965871989727\n","Epoch 1 training loss:\t\t 0.17822435364740524 . Time = 1475.5041689872742 seconds.\n","Training accuracy: 0.5083989104116223\n","100 batches eval. Avg loss:\t 0.14149829417467116 . Avg ms/step = 668.4231209754944\n","Epoch 1 validation loss:\t 0.1524959081927171 . Time = 71.15073275566101 seconds.\n","Validation accuracy: 0.4944110576923077\n"]}],"source":["train_losses = []\n","val_losses = []\n","\n","train_acc = []\n","val_acc = []\n","\n","mounted_path = \"INSERT PATH TO /attention_outputs/\" # create a folder to store attention model outputs and provide path here\n","if not os.path.exists(mounted_path):\n","  os.mkdir(mounted_path)\n","\n","# Loop over epochs\n","for epoch in range(epochs):\n","    # Training\n","\n","    start_time = time.time()\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    model.train()\n","    for i, d in enumerate(training_generator):\n","        batch, labels = d\n","        # Transfer to GPU\n","\n","        text = batch['text']['input_ids'].to(device)\n","        text_mask = batch['text']['attention_mask'].bool().to(device)\n","        graph_batch, molecule_mask = graph_batcher_tr(d[0]['molecule']['cid'])\n","        graph_batch = graph_batch.to(device)\n","        molecule_mask = molecule_mask.to(device)\n","\n","\n","        labels = labels.float().to(device)\n","\n","        text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n","        loss, pred = loss_func(text_out, chem_out, labels)\n","        if torch.isnan(loss): zz\n","\n","        running_loss += loss.item()\n","        running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        scheduler.step()\n","\n","        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \"Acc:\", str(running_acc / (i+1)), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","    train_losses.append(running_loss / (i+1))\n","    train_acc.append(running_acc / (i+1))\n","\n","    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","    print(\"Training accuracy:\", train_acc[-1])\n","\n","\n","    # Validation\n","    model.eval()\n","    with torch.set_grad_enabled(False):\n","      start_time = time.time()\n","      running_acc = 0.0\n","      running_loss = 0.0\n","      for i, d in enumerate(validation_generator):\n","          batch, labels = d\n","          # Transfer to GPU\n","\n","          text = batch['text']['input_ids'].to(device)\n","          text_mask = batch['text']['attention_mask'].bool().to(device)\n","          graph_batch, molecule_mask = graph_batcher_val(d[0]['molecule']['cid'])\n","          graph_batch = graph_batch.to(device)\n","          molecule_mask = molecule_mask.to(device)\n","\n","          labels = labels.float().to(device)\n","\n","          text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n","          loss, pred = loss_func(text_out, chem_out, labels)\n","\n","          running_loss += loss.item()\n","          running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n","\n","\n","          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n","      val_losses.append(running_loss / (i+1))\n","      val_acc.append(running_acc / (i+1))\n","\n","\n","      min_loss = np.min(val_losses)\n","      if val_losses[-1] == min_loss:\n","          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n","\n","    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n","    print(\"Validation accuracy:\", val_acc[-1])\n","\n","\n","torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"]},{"cell_type":"markdown","metadata":{"id":"GYHuY5GkbmfK"},"source":["#Extract attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDmzJpf8boQS"},"outputs":[],"source":["last_decoder = model.text_transformer_decoder.layers[-1]\n","\n","mha_weights = {}\n","def get_activation(name):\n","    def hook(model, input, output):\n","      print(\"YES\")\n","      print(output[1])\n","      mha_weights[cid] = output[1].cpu().detach().numpy()\n","\n","    return hook\n","\n","\n","handle = last_decoder.multihead_attn.register_forward_hook(get_activation(''))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVRbgPCZbsN1"},"outputs":[],"source":["for i,d in enumerate(gt.generate_examples_train()):\n","\n","  batch = d['input']\n","\n","  cid = d['cid']#batch['molecule']['cid'][0]\n","  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n","  graph_batch, molecule_mask = graph_batcher_val([batch['molecule']['cid']])\n","  graph_batch = graph_batch.to(device)\n","  molecule_mask = molecule_mask.to(device)\n","  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","\n","  out = model(text, graph_batch, text_mask, molecule_mask) # will only work on A100 GPU...i tried on free colab, it doesnt work...works only on colab pro+\n","\n","  #for memory reasons\n","  mol_length = graph_batch.x.shape[0]\n","  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n","                                    max_length=gt.text_trunc_length - 1)\n","  text_length = np.sum(text_input['attention_mask'])\n","\n","  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n","\n","  if (i+1) % 1000 == 0: print(i+1)\n","\n","for i,d in enumerate(gt.generate_examples_val()):\n","\n","  batch = d['input']\n","\n","  cid = d['cid']#batch['molecule']['cid'][0]\n","  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n","  graph_batch, molecule_mask = graph_batcher_val([batch['molecule']['cid']])\n","  graph_batch = graph_batch.to(device)\n","  molecule_mask = molecule_mask.to(device)\n","  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","\n","\n","  out = model(text, graph_batch, text_mask, molecule_mask)\n","\n","  #for memory reasons\n","  mol_length = graph_batch.x.shape[0]\n","  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n","                                    max_length=gt.text_trunc_length - 1)\n","  text_length = np.sum(text_input['attention_mask'])\n","  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n","\n","\n","  if (i+1) % 1000 == 0: print(i+1)\n","\n","for i,d in enumerate(gt.generate_examples_test()):\n","\n","  batch = d['input']\n","\n","  cid = d['cid']\n","  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n","\n","  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n","  graph_batch, molecule_mask = graph_batcher_test([batch['molecule']['cid']])\n","  graph_batch = graph_batch.to(device)\n","  molecule_mask = molecule_mask.to(device)\n","  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n","\n","\n","  out = model(text, graph_batch, text_mask, molecule_mask)\n","\n","  #for memory reasons\n","  mol_length = graph_batch.x.shape[0]\n","  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n","                                    max_length=gt.text_trunc_length - 1)\n","  text_length = np.sum(text_input['attention_mask'])\n","  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n","\n","\n","  if (i+1) % 1000 == 0: print(i+1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKRm1JIGbzE6"},"outputs":[],"source":["import pickle\n","\n","path = \"INSERT PATH TO /attention_input_wt/\"\n","with open(path + \"mha_weights.pkl\", 'wb') as fp:\n","  pickle.dump(mha_weights, fp)"]},{"cell_type":"code","source":[],"metadata":{"id":"nGXC9unNWQRk"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7a4173cbdf3c4951bee97344a9df41bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f99cac77cffb492a9b1b4f922413e022","IPY_MODEL_617bce91a742494eaefc97565046f254","IPY_MODEL_2c390503a53c4d859fc9eff9a0f25c85"],"layout":"IPY_MODEL_6a1c22c9b0364337b963106c0809e82b"}},"f99cac77cffb492a9b1b4f922413e022":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7385d5a059d4fc9a624e644eb88c9fb","placeholder":"​","style":"IPY_MODEL_5ae2572873754f14ac68168315dea5c7","value":"vocab.txt: 100%"}},"617bce91a742494eaefc97565046f254":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceb5ff331396400493a8b9b8d3332435","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1657dded2b1b4a62947a742f11c1c324","value":227845}},"2c390503a53c4d859fc9eff9a0f25c85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb35a69b659645a7a53898c7191dc389","placeholder":"​","style":"IPY_MODEL_db4227adec5d408286e54caf72655e79","value":" 228k/228k [00:00&lt;00:00, 2.89MB/s]"}},"6a1c22c9b0364337b963106c0809e82b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7385d5a059d4fc9a624e644eb88c9fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ae2572873754f14ac68168315dea5c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceb5ff331396400493a8b9b8d3332435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1657dded2b1b4a62947a742f11c1c324":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb35a69b659645a7a53898c7191dc389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db4227adec5d408286e54caf72655e79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"263f594fcac64ba298e9f2232e411b8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d83701e239f4a9b9fec674f12b27480","IPY_MODEL_3c04d50cd546454296bdffe43e28e48b","IPY_MODEL_20585ed0a9e845ba832098d5b403431e"],"layout":"IPY_MODEL_4b7f8ccf48eb4f9aad034d0705b8f247"}},"1d83701e239f4a9b9fec674f12b27480":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d503127fad7a4b809d87de27eef1b757","placeholder":"​","style":"IPY_MODEL_7dccc5e7dd1745dc885047fa65ca0c28","value":"config.json: 100%"}},"3c04d50cd546454296bdffe43e28e48b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06bce96e63894f3492e70308532dbd77","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bc1149e590344048d5a9488109d0e18","value":385}},"20585ed0a9e845ba832098d5b403431e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_478d9bb35b2c4839b82a37457ebf1dc4","placeholder":"​","style":"IPY_MODEL_107592c9fc1b444fa03c01395c7f4900","value":" 385/385 [00:00&lt;00:00, 4.57kB/s]"}},"4b7f8ccf48eb4f9aad034d0705b8f247":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d503127fad7a4b809d87de27eef1b757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dccc5e7dd1745dc885047fa65ca0c28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06bce96e63894f3492e70308532dbd77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bc1149e590344048d5a9488109d0e18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"478d9bb35b2c4839b82a37457ebf1dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"107592c9fc1b444fa03c01395c7f4900":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}